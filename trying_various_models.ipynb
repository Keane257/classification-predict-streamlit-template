{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trying_various_models.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicafar147/classification-predict-streamlit-template/blob/Modeling/trying_various_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZFQhYTWsgZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "70caeca2-adf9-4e6e-ecef-ef78515dd56d"
      },
      "source": [
        "!pip install comet_ml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting comet_ml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/27/bce4a8989b9eb0229294b844086e584d450af883ddac65286246312dc412/comet_ml-3.1.11-py2.py3-none-any.whl (212kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 4.7MB/s \n",
            "\u001b[?25hCollecting netifaces>=0.10.7\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/24/5e/f3bd8443bfdf96d2f5d10097d301076a9eb55637b7864e52d2d1a4d8c72a/wurlitzer-2.0.0-py2.py3-none-any.whl\n",
            "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.23.0)\n",
            "Collecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Collecting comet-git-pure>=0.19.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7a/483413046e48908986a0f9a1d8a917e1da46ae58e6ba16b2ac71b3adf8d7/comet_git_pure-0.19.16-py3-none-any.whl (409kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 11.2MB/s \n",
            "\u001b[?25hCollecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2020.4.5.2)\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34546 sha256=7404134c1341239f353a9083d3227478ab0225629bfb06f835c0f1694a1fb514\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built configobj\n",
            "Installing collected packages: netifaces, wurlitzer, configobj, everett, websocket-client, comet-git-pure, comet-ml\n",
            "Successfully installed comet-git-pure-0.19.16 comet-ml-3.1.11 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9 websocket-client-0.57.0 wurlitzer-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP39xluOsk_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "aa3f986b-90d9-4092-fb3d-26b0b575f1f1"
      },
      "source": [
        "# import comet_ml in the top of your file\n",
        "from comet_ml import Experiment\n",
        "    \n",
        "# Add the following code anywhere in your machine learning file\n",
        "experiment = Experiment(api_key=\"rBqQ3hDuEa6xVpT9ns5Tz1dVt\",project_name=\"nlp-climate-change\", workspace=\"monicafar147\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/monicafar147/nlp-climate-change/39a80c700148442781be1ed3cae130fe\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TVMI3wj7sB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d70a442b-fee0-447e-913c-b0e05016361c"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_e9WjyW7zRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3f4dbefc-011a-4960-f8c4-f120756fa4bf"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-deep')\n",
        "\n",
        "# text preprocessing\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import Word\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# models\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZY3j6fc7zZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/monicafar147/classification-predict-streamlit-template/master/climate-change-belief-analysis/train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/monicafar147/classification-predict-streamlit-template/master/climate-change-belief-analysis/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0PZTMaE7zcn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5f0c1df4-7d14-4235-cfac-492524ed160f"
      },
      "source": [
        "print(\"Train\\n\")\n",
        "print(train.head(5))\n",
        "print(\"\\nTest\")\n",
        "print(test.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "\n",
            "   sentiment                                            message  tweetid\n",
            "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
            "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
            "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
            "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
            "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954\n",
            "\n",
            "Test\n",
            "                                             message  tweetid\n",
            "0  Europe will now be looking to China to make su...   169760\n",
            "1  Combine this with the polling of staffers re c...    35326\n",
            "2  The scary, unimpeachable evidence that climate...   224985\n",
            "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
            "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H59B5hNC7zfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _preprocess(data):\n",
        "  df = data.copy()\n",
        "\n",
        "  # apply lowercase to data\n",
        "  data['message'] = data['message'].apply(lambda word: ''.join(word.lower()))\n",
        "\n",
        "  # function to remove contraction\n",
        "  def remove_contraction(row):\n",
        "    fixed = [contractions.fix(word) for word in row.split()]\n",
        "    return ' '.join(map(str,fixed))\n",
        "\n",
        "  # replace contractions\n",
        "  df['message'] = np.vectorize(remove_contraction)(df['message'])\n",
        "\n",
        "  # function to remove patterns\n",
        "  def remove_pattern(text,pattern,replacement=''):\n",
        "    remove_this = re.findall(pattern, text)\n",
        "    for item in remove_this:\n",
        "      text = re.sub(item, replacement, text)\n",
        "    return text\n",
        "\n",
        "  # remove URL\n",
        "  df['message'] = df['message'].apply(lambda word: re.split('https:\\/\\/.*', str(word))[0])\n",
        "\n",
        "  # remove punctuation\n",
        "  df['message'] = df['message'].apply(lambda word: word.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "  # remove stopwords\n",
        "  stop_words = stopwords.words('english')\n",
        "  data['message'] = data['message'].apply(lambda word: ' '.join(word for word in word.split() if word not in stop_words))\n",
        "\n",
        "  # remove retweet as rt\n",
        "  df['message'] = np.vectorize(remove_pattern)(df['message'],\"RT[\\w]*\")\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq3_kotW7zhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained = train[['sentiment','message','tweetid']]\n",
        "tested = test[['message','tweetid']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvejzL-O8aZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_processed = _preprocess(trained)\n",
        "test_processed = _preprocess(tested)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOgreYHI8acf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting the labels and features\n",
        "X = train_processed['message']\n",
        "y = train_processed['sentiment']\n",
        "\n",
        "# Splitting the labels and fetures into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxRIdIciNl9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "707437e4-e152-4b47-cf4c-eec15104aada"
      },
      "source": [
        "# Decision tree\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import tree\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "text_cls = Pipeline([('tfidf',TfidfVectorizer()),('classify',clf)])\n",
        "text_cls.fit(X_train, y_train)\n",
        "\n",
        "pred = text_cls.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "report_1 = print(classification_report(y_test, pred))\n",
        "report_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.36      0.27      0.31       126\n",
            "           0       0.34      0.33      0.33       224\n",
            "           1       0.72      0.72      0.72       895\n",
            "           2       0.58      0.67      0.62       337\n",
            "\n",
            "    accuracy                           0.62      1582\n",
            "   macro avg       0.50      0.50      0.50      1582\n",
            "weighted avg       0.61      0.62      0.61      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET-j-S-eNmAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "42c63009-0325-4069-cec8-a8be877edea8"
      },
      "source": [
        "# Linear SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC()\n",
        "text_svc = Pipeline([('tfidf',TfidfVectorizer()),('classify',svc)])\n",
        "text_svc.fit(X_train, y_train)\n",
        "\n",
        "pred_2 = text_svc.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "report_1 = print(classification_report(y_test, pred_2))\n",
        "report_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.64      0.49      0.56       126\n",
            "           0       0.58      0.35      0.44       224\n",
            "           1       0.79      0.86      0.82       895\n",
            "           2       0.72      0.78      0.75       337\n",
            "\n",
            "    accuracy                           0.74      1582\n",
            "   macro avg       0.68      0.62      0.64      1582\n",
            "weighted avg       0.73      0.74      0.73      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeA-ot4LNmC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e05f6527-e656-4552-a691-bd812bd2067e"
      },
      "source": [
        "# Gaussian not working though\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import todense\n",
        "naive_bayes = GaussianNB()\n",
        "text_nb = Pipeline([('tfidf',TfidfVectorizer()),('classify',naive_bayes)])\n",
        "XD_train = X_train.todense()\n",
        "text_nb.fit(XD_train, y_train)\n",
        "\n",
        "pred_3 = text_nb.predict(X_test)\n",
        "report_1 = print(classification_report(y_test, pred_3))\n",
        "report_1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-1a1412b3b3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtodense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnaive_bayes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classify'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'todense'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCJx2j2fRfas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a882b0b2-44bf-4b0f-ed6b-fed9987cebe9"
      },
      "source": [
        "# K nearest neighbours, 250 seems best but maybe could be a better option\n",
        "n_neighbors = 250 \n",
        "knn = KNeighborsClassifier(n_neighbors)\n",
        "text_knn = Pipeline([('tfidf',TfidfVectorizer()),('classify',knn)])\n",
        "text_knn.fit(X_train, y_train)\n",
        "\n",
        "pred_4 = text_knn.predict(X_test)\n",
        "report_1 = print(classification_report(y_test, pred_4))\n",
        "report_1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.01      0.02       126\n",
            "           0       0.88      0.03      0.06       224\n",
            "           1       0.63      0.97      0.76       895\n",
            "           2       0.81      0.44      0.57       337\n",
            "\n",
            "    accuracy                           0.65      1582\n",
            "   macro avg       0.83      0.36      0.35      1582\n",
            "weighted avg       0.73      0.65      0.56      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfHGJX87Rfd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "820acbd4-acaf-48a3-fc5c-92dcfe06981f"
      },
      "source": [
        "# MulitnomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB()\n",
        "text_mnb = Pipeline([('tfidf',TfidfVectorizer()),('classify',model)])\n",
        "text_mnb.fit(X_train, y_train)\n",
        "pred_5 = text_mnb.predict(X_test)\n",
        "report_5 = print(classification_report(y_test, pred_5))\n",
        "report_5\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.01      0.02       126\n",
            "           0       1.00      0.03      0.06       224\n",
            "           1       0.62      0.99      0.76       895\n",
            "           2       0.90      0.42      0.57       337\n",
            "\n",
            "    accuracy                           0.65      1582\n",
            "   macro avg       0.88      0.36      0.35      1582\n",
            "weighted avg       0.77      0.65      0.56      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd8G4pI1RfhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "4de8a797-7b71-43ef-a542-56b0cb831310"
      },
      "source": [
        "# Logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "text_lr = Pipeline([('tfidf',TfidfVectorizer()),('classify',lr)])\n",
        "text_lr.fit(X_train, y_train)\n",
        "pred_6 = text_lr.predict(X_test)\n",
        "report_6 = print(classification_report(y_test, pred_6))\n",
        "report_6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.79      0.36      0.49       126\n",
            "           0       0.64      0.31      0.42       224\n",
            "           1       0.75      0.91      0.82       895\n",
            "           2       0.76      0.75      0.76       337\n",
            "\n",
            "    accuracy                           0.75      1582\n",
            "   macro avg       0.74      0.58      0.62      1582\n",
            "weighted avg       0.74      0.75      0.73      1582\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgGfQHN5NmJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}