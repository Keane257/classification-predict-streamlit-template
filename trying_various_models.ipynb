{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trying_various_models.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicafar147/classification-predict-streamlit-template/blob/Modeling/trying_various_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZFQhYTWsgZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6852cdc9-75dd-4a59-e683-75dd886c491e"
      },
      "source": [
        "!pip install comet_ml"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.6/dist-packages (3.1.11)\n",
            "Requirement already satisfied: netifaces>=0.10.7 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.10.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Requirement already satisfied: comet-git-pure>=0.19.11 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.19.16)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.0.0)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.0.2)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.57.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: configobj; extra == \"ini\" in /usr/local/lib/python3.6/dist-packages (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml) (5.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP39xluOsk_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "71f81dcb-06e8-45dc-b6ca-0a929c5d3ae9"
      },
      "source": [
        "# import comet_ml in the top of your file\n",
        "from comet_ml import Experiment\n",
        "    \n",
        "# Add the following code anywhere in your machine learning file\n",
        "experiment = Experiment(api_key=\"rBqQ3hDuEa6xVpT9ns5Tz1dVt\",project_name=\"nlp-climate-change\", workspace=\"monicafar147\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b01021cb69df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add the following code anywhere in your machine learning file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rBqQ3hDuEa6xVpT9ns5Tz1dVt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nlp-climate-change\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"monicafar147\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/comet_ml/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, project_name, workspace, log_code, log_graph, auto_param_logging, auto_metric_logging, parse_args, auto_output_logging, log_env_details, log_git_metadata, log_git_patch, disabled, log_env_gpu, log_env_host, display_summary, log_env_cpu, display_summary_level)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mlog_env_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_env_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mdisplay_summary_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_summary_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mlog_env_cpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_env_cpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/comet_ml/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, project_name, workspace, log_code, log_graph, auto_param_logging, auto_metric_logging, parse_args, auto_output_logging, log_env_details, log_git_metadata, log_git_patch, disabled, log_env_gpu, log_env_host, display_summary, log_env_cpu, display_summary_level)\u001b[0m\n\u001b[1;32m    281\u001b[0m                     \u001b[0mALREADY_IMPORTED_MODULES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 )\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m# Generate a unique identifier for this experiment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: You must import Comet before these modules: sklearn",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TVMI3wj7sB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d70a442b-fee0-447e-913c-b0e05016361c"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_e9WjyW7zRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3f4dbefc-011a-4960-f8c4-f120756fa4bf"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-deep')\n",
        "\n",
        "# text preprocessing\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import Word\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# models\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZY3j6fc7zZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/monicafar147/classification-predict-streamlit-template/master/climate-change-belief-analysis/train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/monicafar147/classification-predict-streamlit-template/master/climate-change-belief-analysis/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0PZTMaE7zcn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5f0c1df4-7d14-4235-cfac-492524ed160f"
      },
      "source": [
        "print(\"Train\\n\")\n",
        "print(train.head(5))\n",
        "print(\"\\nTest\")\n",
        "print(test.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "\n",
            "   sentiment                                            message  tweetid\n",
            "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
            "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
            "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
            "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
            "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954\n",
            "\n",
            "Test\n",
            "                                             message  tweetid\n",
            "0  Europe will now be looking to China to make su...   169760\n",
            "1  Combine this with the polling of staffers re c...    35326\n",
            "2  The scary, unimpeachable evidence that climate...   224985\n",
            "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
            "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H59B5hNC7zfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _preprocess(data):\n",
        "  df = data.copy()\n",
        "\n",
        "  # apply lowercase to data\n",
        "  data['message'] = data['message'].apply(lambda word: ''.join(word.lower()))\n",
        "\n",
        "  # function to remove contraction\n",
        "  def remove_contraction(row):\n",
        "    fixed = [contractions.fix(word) for word in row.split()]\n",
        "    return ' '.join(map(str,fixed))\n",
        "\n",
        "  # replace contractions\n",
        "  df['message'] = np.vectorize(remove_contraction)(df['message'])\n",
        "\n",
        "  # function to remove patterns\n",
        "  def remove_pattern(text,pattern,replacement=''):\n",
        "    remove_this = re.findall(pattern, text)\n",
        "    for item in remove_this:\n",
        "      text = re.sub(item, replacement, text)\n",
        "    return text\n",
        "\n",
        "  # remove URL\n",
        "  df['message'] = df['message'].apply(lambda word: re.split('https:\\/\\/.*', str(word))[0])\n",
        "\n",
        "  # remove punctuation\n",
        "  df['message'] = df['message'].apply(lambda word: word.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "  # remove stopwords\n",
        "  stop_words = stopwords.words('english')\n",
        "  data['message'] = data['message'].apply(lambda word: ' '.join(word for word in word.split() if word not in stop_words))\n",
        "\n",
        "  # remove retweet as rt\n",
        "  df['message'] = np.vectorize(remove_pattern)(df['message'],\"RT[\\w]*\")\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq3_kotW7zhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained = train[['sentiment','message','tweetid']]\n",
        "tested = test[['message','tweetid']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvejzL-O8aZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_processed = _preprocess(trained)\n",
        "test_processed = _preprocess(tested)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOgreYHI8acf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting the labels and features\n",
        "X = train_processed['message']\n",
        "y = train_processed['sentiment']\n",
        "\n",
        "# Splitting the labels and fetures into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxRIdIciNl9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "707437e4-e152-4b47-cf4c-eec15104aada"
      },
      "source": [
        "# Decision tree\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import tree\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "text_cls = Pipeline([('tfidf',TfidfVectorizer()),('classify',clf)])\n",
        "text_cls.fit(X_train, y_train)\n",
        "\n",
        "pred = text_cls.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "report_1 = print(classification_report(y_test, pred))\n",
        "report_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.36      0.27      0.31       126\n",
            "           0       0.34      0.33      0.33       224\n",
            "           1       0.72      0.72      0.72       895\n",
            "           2       0.58      0.67      0.62       337\n",
            "\n",
            "    accuracy                           0.62      1582\n",
            "   macro avg       0.50      0.50      0.50      1582\n",
            "weighted avg       0.61      0.62      0.61      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IukrgntuvWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionaries for the data we want to log\n",
        "\n",
        "params = {\"preprocessing\":  \"_preprocess(data)\",\n",
        "          \"model_type\": \"DecisionTreeClassifier\",\n",
        "          }\n",
        "\n",
        "metrics = {\"report\" : report_1,\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET-j-S-eNmAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "42c63009-0325-4069-cec8-a8be877edea8"
      },
      "source": [
        "# Linear SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC()\n",
        "text_svc = Pipeline([('tfidf',TfidfVectorizer()),('classify',svc)])\n",
        "text_svc.fit(X_train, y_train)\n",
        "\n",
        "pred_2 = text_svc.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "report_2 = print(classification_report(y_test, pred_2))\n",
        "report_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.64      0.49      0.56       126\n",
            "           0       0.58      0.35      0.44       224\n",
            "           1       0.79      0.86      0.82       895\n",
            "           2       0.72      0.78      0.75       337\n",
            "\n",
            "    accuracy                           0.74      1582\n",
            "   macro avg       0.68      0.62      0.64      1582\n",
            "weighted avg       0.73      0.74      0.73      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOhxYBLfvA_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionaries for the data we want to log\n",
        "\n",
        "params = {\"preprocessing\":  \"_preprocess(data)\",\n",
        "          \"model_type\": \"LinearSVC\",\n",
        "          }\n",
        "\n",
        "metrics = {\"report\" : report_2,\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeA-ot4LNmC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e05f6527-e656-4552-a691-bd812bd2067e"
      },
      "source": [
        "# Gaussian not working though\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import todense\n",
        "naive_bayes = GaussianNB()\n",
        "text_nb = Pipeline([('tfidf',TfidfVectorizer()),('classify',naive_bayes)])\n",
        "XD_train = X_train.todense()\n",
        "text_nb.fit(XD_train, y_train)\n",
        "\n",
        "pred_3 = text_nb.predict(X_test)\n",
        "report_3 = print(classification_report(y_test, pred_3))\n",
        "report_3\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-1a1412b3b3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtodense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnaive_bayes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classify'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'todense'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCJx2j2fRfas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a882b0b2-44bf-4b0f-ed6b-fed9987cebe9"
      },
      "source": [
        "# K nearest neighbours, 250 seems best but maybe could be a better option\n",
        "n_neighbors = 250 \n",
        "knn = KNeighborsClassifier(n_neighbors)\n",
        "text_knn = Pipeline([('tfidf',TfidfVectorizer()),('classify',knn)])\n",
        "text_knn.fit(X_train, y_train)\n",
        "\n",
        "pred_4 = text_knn.predict(X_test)\n",
        "report_4 = print(classification_report(y_test, pred_4))\n",
        "report_4\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.01      0.02       126\n",
            "           0       0.88      0.03      0.06       224\n",
            "           1       0.63      0.97      0.76       895\n",
            "           2       0.81      0.44      0.57       337\n",
            "\n",
            "    accuracy                           0.65      1582\n",
            "   macro avg       0.83      0.36      0.35      1582\n",
            "weighted avg       0.73      0.65      0.56      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-QkZ5-7vTYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\"preprocessing\":  \"_preprocess(data)\",\n",
        "          \"model_type\": \"KNeighborsClassifier\",\n",
        "          }\n",
        "\n",
        "metrics = {\"report\" : report_4,\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfHGJX87Rfd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "820acbd4-acaf-48a3-fc5c-92dcfe06981f"
      },
      "source": [
        "# MulitnomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB()\n",
        "text_mnb = Pipeline([('tfidf',TfidfVectorizer()),('classify',model)])\n",
        "text_mnb.fit(X_train, y_train)\n",
        "pred_5 = text_mnb.predict(X_test)\n",
        "report_5 = print(classification_report(y_test, pred_5))\n",
        "report_5\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.01      0.02       126\n",
            "           0       1.00      0.03      0.06       224\n",
            "           1       0.62      0.99      0.76       895\n",
            "           2       0.90      0.42      0.57       337\n",
            "\n",
            "    accuracy                           0.65      1582\n",
            "   macro avg       0.88      0.36      0.35      1582\n",
            "weighted avg       0.77      0.65      0.56      1582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrQzgO9wveSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\"preprocessing\":  \"_preprocess(data)\",\n",
        "          \"model_type\": \"MultinomialNB\",\n",
        "          }\n",
        "\n",
        "metrics = {\"report\" : report_5,\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd8G4pI1RfhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "4de8a797-7b71-43ef-a542-56b0cb831310"
      },
      "source": [
        "# Logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "text_lr = Pipeline([('tfidf',TfidfVectorizer()),('classify',lr)])\n",
        "text_lr.fit(X_train, y_train)\n",
        "pred_6 = text_lr.predict(X_test)\n",
        "report_6 = print(classification_report(y_test, pred_6))\n",
        "report_6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.79      0.36      0.49       126\n",
            "           0       0.64      0.31      0.42       224\n",
            "           1       0.75      0.91      0.82       895\n",
            "           2       0.76      0.75      0.76       337\n",
            "\n",
            "    accuracy                           0.75      1582\n",
            "   macro avg       0.74      0.58      0.62      1582\n",
            "weighted avg       0.74      0.75      0.73      1582\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb8Hj24wvmRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\"preprocessing\":  \"_preprocess(data)\",\n",
        "          \"model_type\": \"LogisticRegression\",\n",
        "          }\n",
        "\n",
        "metrics = {\"report\" : report_6,\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgGfQHN5NmJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}